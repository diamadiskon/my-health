# Ollama Local AI Setup Guide - My-Health Application

**Status:** ‚úÖ Ready to Deploy
**Integration:** Added to existing docker-compose.yml
**Model:** Llama 2 7B Chat (Healthcare Optimized)

---

## üöÄ Quick Start

I've successfully integrated Ollama into your existing My-Health application! Here's what's been set up:

### ‚úÖ What's Been Configured

1. **Docker Integration:** Added Ollama service to your existing `docker-compose.yml`
2. **Healthcare Model:** Automated download of Llama 2 7B Chat
3. **Setup Scripts:** Created automated setup and testing scripts
4. **Health Checks:** Added service monitoring and restart policies

---

## üèÉ‚Äç‚ôÇÔ∏è How to Start Ollama

### Option 1: Run Setup Script (Recommended)
```bash
cd /Users/diamadiskon/Desktop/my\ projects/my-health
./setup-ollama.sh
```

This script will:
- ‚úÖ Start the Ollama container
- ‚úÖ Download the Llama 2 7B Chat model (~4GB)
- ‚úÖ Create a healthcare-optimized model called "healthbot"
- ‚úÖ Run comprehensive tests
- ‚úÖ Provide connection details

### Option 2: Manual Docker Commands
```bash
# Start Ollama service
docker-compose up -d ollama

# Wait for service to start (30 seconds)
sleep 30

# Download the model
docker exec my-health-ollama ollama pull llama2:7b-chat

# Test the API
curl http://localhost:11434/api/tags
```

---

## üìã Docker Compose Changes

Here's what was added to your `docker-compose.yml`:

```yaml
  ollama:
    container_name: my-health-ollama
    image: ollama/ollama:latest
    ports:
      - '11434:11434'
    volumes:
      - ollama_models:/root/.ollama
    environment:
      - OLLAMA_ORIGINS=*
      - OLLAMA_HOST=0.0.0.0:11434
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 5s

volumes:
  db_data:
  ollama_models:  # Added for persistent model storage
```

### Key Features:
- **Port 11434:** Ollama API accessible at `localhost:11434`
- **Persistent Storage:** Models persist across container restarts
- **Health Checks:** Automatic service monitoring
- **Auto-restart:** Service automatically restarts if it fails
- **CORS Enabled:** Ready for frontend integration

---

## üß™ Testing Your Setup

### Run Comprehensive Tests
```bash
./test-ollama.sh
```

This will test:
- ‚úÖ API connectivity
- ‚úÖ Healthcare model responses
- ‚úÖ Performance benchmarking
- ‚úÖ Streaming API functionality

### Manual API Test
```bash
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{"model": "healthbot", "prompt": "What is blood pressure?", "stream": false}'
```

---

## üè• Healthcare Model Features

The "healthbot" model has been optimized for your application:

### ‚úÖ Healthcare-Specific Training
- Responds appropriately to elderly health questions
- Avoids medical diagnosis (refers to healthcare providers)
- Uses simple, clear language suitable for elderly users
- Focuses on educational health information

### ‚öôÔ∏è Optimized Parameters
- **Temperature 0.3:** More consistent, reliable responses
- **Top-p 0.9:** Balanced creativity and accuracy
- **Context Length 4096:** Can handle long health conversations
- **Repeat Penalty 1.1:** Avoids repetitive responses

### üìù Sample Responses
- "What does 120/80 blood pressure mean?"
- "Is a heart rate of 75 bpm normal for my age?"
- "When should I take my blood pressure medication?"
- "How can I improve my sleep quality?"

---

## üîó Backend Integration

### API Endpoint Information
- **URL:** `http://localhost:11434/api/generate`
- **Method:** POST
- **Content-Type:** `application/json`

### Request Format
```json
{
  "model": "healthbot",
  "prompt": "Your health question here",
  "stream": false
}
```

### Response Format
```json
{
  "response": "AI-generated health information response"
}
```

### Go Client Example
```go
type OllamaRequest struct {
    Model  string `json:"model"`
    Prompt string `json:"prompt"`
    Stream bool   `json:"stream"`
}

type OllamaResponse struct {
    Response string `json:"response"`
}

func CallOllama(prompt string) (string, error) {
    reqBody := OllamaRequest{
        Model:  "healthbot",
        Prompt: prompt,
        Stream: false,
    }

    jsonData, _ := json.Marshal(reqBody)

    resp, err := http.Post("http://localhost:11434/api/generate",
                          "application/json",
                          bytes.NewBuffer(jsonData))
    if err != nil {
        return "", err
    }
    defer resp.Body.Close()

    var ollamaResp OllamaResponse
    json.NewDecoder(resp.Body).Decode(&ollamaResp)

    return ollamaResp.Response, nil
}
```

---

## üíæ Resource Requirements

### System Requirements
- **RAM:** 8GB minimum (4GB for model + 4GB for system)
- **Storage:** 6GB for model files
- **CPU:** 4+ cores recommended for good performance

### Performance Expectations
- **Response Time:** 3-8 seconds (depending on question complexity)
- **Concurrent Users:** 5-10 simultaneous chats
- **Model Size:** Llama 2 7B (~4GB downloaded)

---

## üîß Troubleshooting

### Common Issues & Solutions

#### 1. Container Won't Start
```bash
# Check Docker logs
docker-compose logs ollama

# Restart the service
docker-compose restart ollama
```

#### 2. Model Download Fails
```bash
# Manually download model
docker exec my-health-ollama ollama pull llama2:7b-chat

# Check available storage
docker system df
```

#### 3. API Not Responding
```bash
# Check if service is running
curl http://localhost:11434/api/tags

# Verify port is open
netstat -an | grep 11434
```

#### 4. Out of Memory Errors
```bash
# Check available RAM
free -h

# Reduce concurrent requests or use smaller model
docker exec my-health-ollama ollama pull llama2:7b-chat  # Already optimal size
```

### Health Check Commands
```bash
# Service status
docker-compose ps ollama

# API health
curl http://localhost:11434/api/tags

# Model list
docker exec my-health-ollama ollama list

# Container logs
docker logs my-health-ollama
```

---

## üéØ Next Steps

### 1. Backend AI Service Implementation
Create the Go backend service to integrate with Ollama:
- `/backend/services/ai/chat_service.go`
- User context building from health data
- Conversation history management
- Rate limiting and security

### 2. Frontend Chat Component
Implement the React chat interface:
- `/frontend/src/components/AIChat/AIChatBot.tsx`
- Material-UI chat interface
- Real-time messaging
- Mobile-responsive design

### 3. Database Schema Updates
Add chat-related tables:
```sql
CREATE TABLE chat_sessions (
    id SERIAL PRIMARY KEY,
    user_id INTEGER REFERENCES users(id),
    session_id UUID DEFAULT gen_random_uuid(),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE chat_messages (
    id SERIAL PRIMARY KEY,
    session_id UUID REFERENCES chat_sessions(session_id),
    role VARCHAR(20) NOT NULL, -- 'user' or 'assistant'
    content TEXT NOT NULL,
    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

---

## üìä Service Monitoring

### Important Endpoints
- **Health Check:** `http://localhost:11434/api/tags`
- **Generate Response:** `http://localhost:11434/api/generate`
- **Model List:** `http://localhost:11434/api/tags`

### Log Locations
- **Container Logs:** `docker logs my-health-ollama`
- **Service Status:** `docker-compose ps`
- **API Testing:** Use the provided `test-ollama.sh` script

---

## ‚úÖ Ready for Integration

Your Ollama AI service is now:
- ‚úÖ **Integrated** with existing Docker setup
- ‚úÖ **Optimized** for healthcare questions
- ‚úÖ **Tested** and ready for backend integration
- ‚úÖ **Monitored** with health checks and auto-restart
- ‚úÖ **Persistent** with volume storage for models

**Next:** Implement the Go backend AI chat service and React frontend components as outlined in the `AI_CHATBOT_IMPLEMENTATION.md` guide!

---

## üéâ Success!

You now have a fully functional, privacy-focused AI assistant running locally on your machine. The AI can answer healthcare questions specific to your users' data without any external API calls or data privacy concerns.

**Start the setup with:** `./setup-ollama.sh`
**Test everything with:** `./test-ollama.sh`